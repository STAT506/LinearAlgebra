---
title: "Linear Algebra Primer"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```

##### Matrices / Vectors

A matrix is an $n \times p$ object. Matrices are often denoted by a capital letter (or Greek symbol). A few common matrices will be

$$X = \begin{pmatrix}
1 & x_{11} & x_{12}\\
1 & x_{21} & x_{22}\\
\vdots & \vdots & \vdots \\
1 & x_{n1} & x_{n2}
\end{pmatrix}$$

or

$$\Sigma = \begin{pmatrix}
\sigma^2_1 & \sigma_{12} & \cdots &\sigma_{1n}\\
\sigma_{21} & \sigma^2_{2} & \cdots &\sigma_{2n}\\
\vdots & \vdots & \ddots &\vdots\\
\sigma_{n1} & \sigma_{n2} & \ddots &\sigma^2_{n}\\
\end{pmatrix}$$

or

$$J_n = \begin{pmatrix}
1 & 1 & \cdots &1\\
1 & 1 & \cdots &1\\
\vdots & \vdots & \ddots &\vdots\\
1 & 1 & \ddots &1\\
\end{pmatrix}$$

or

$$I_n = \begin{pmatrix}
1 & 0 & \cdots &0\\
0 & 1 & \cdots &0\\
\vdots & \vdots & \ddots &\vdots\\
0 & 0 & \ddots &1\\
\end{pmatrix}$$

\vfill


\newpage

Vectors are essentially one-dimension vectors and will be denoted with an underline. We will assume vectors are $q \times 1$ dimension unless noted with a transpose.

$$\underline{y} = \begin{pmatrix}
  y_1 \\
  y_2 \\
  \vdots\\
  y_n \end{pmatrix}$$

or

$$\underline{\beta} = \begin{pmatrix}
  \beta_0 \\
  \beta_1 \\
  \vdots\\
  \beta_{p-1} \end{pmatrix}$$

or

$$\underline{1}_n = \begin{pmatrix}
  1 \\
  1 \\
  \vdots\\
  1 \end{pmatrix}$$

\vfill

The transpose operator will be denoted by $\underline{y}^T = \begin{pmatrix} y_1 & y_2 & \cdots & y_n \end{pmatrix}$ or $\underline{y}^{'}$, both of which would result in a $1 \times n$ vector.

\vfill
\newpage

##### Matrix Multiplication

The most important component in matrix multiplication is tracking dimensions.

\vfill

Consider a simple case with 

$$\hat{\underline{y}} = X \times \hat{\underline{\beta}},$$

where $X$ is a $2 \times 2$ matrix, $\begin{pmatrix} 1 & 2 \\ 1 & -1   \end{pmatrix}$ and $\hat{\underline{\beta}} = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$.

\vfill

Then $$\hat{\underline{y}}= \begin{bmatrix}
1 \times 3 + 2 \times 2 \\
1 \times 3 + (-1)\times 2 \\
\end{bmatrix} = 
\begin{bmatrix}
7 \\
-1 \\
\end{bmatrix}$$

In R, we use `%*%` for matrix multiplication.

```{r}
X <- matrix(c(1,2, 1 ,-1), nrow = 2, ncol = 2, byrow = T); X
#X <- matrix(c(1, 1, 2 ,-1), nrow = 2, ncol = 2); X
beta_hat <- matrix(c(3,2),nrow =2, ncol = 1); beta_hat
```

\vfill

```{r}
y_hat <- X %*% beta_hat; y_hat
```

\vfill

\newpage

#### Kronecker Product

Kronecker product $\otimes$, enables a different type of matrix multiplication.

If $$A = \begin{bmatrix} 1 & 2\end{bmatrix}$$ and $B = \begin{bmatrix} 3 \\ 4\\ 5 \end{bmatrix}$, then

$$A \otimes B = \begin{bmatrix} 3 & 6 \\
4 & 8\\
5 & 10\end{bmatrix}$$

## More about matrices

#### Nonsingular matrix and Matrix Inverse

If X is a square matrix, then X is nonsingular if there exists another matrix s.t.

$X X^{-1} = I$ and $X^{-1} X = I$

\vfill

$X^{-1}$ is the inverse of a matrix. We can calculate the inverse of a matrix for a $1 \times 1$ matrix, perhaps as $2 \times 2$, matrix and maybe even a $3 \times 3$ matrix. However, beyond that it is quite challenging and time consuming. Furthermore, it is also (relatively) time intensive for your computer.

\vfill

#### Orthogonal matrices

If a matrix $X$ has an inverse that is also the transpose, $X X^T = I$, then $X$ is an orthogonal matrix.

\vfill


\newpage

##### Motivating Dataset: Washington (DC) housing dataset

Hopefully the connections to statistics are clear, using $X$ and $\beta$, but let's consider a motivating dataset.

\vfill
This dataset contains housing information from Washington, D.C. It was used for a STAT532 exam, so apologize in advance for any scar tissue.

```{r}
DC <- read_csv('https://math.montana.edu/ahoegh/teaching/stat532/data/DC.csv')
```


```{r}
DC %>% group_by(WARD) %>% 
  summarize(`Average Price (millions of dollars)` = mean(PRICE)/1000000, .groups = 'drop') %>% 
  kable(digits = 3)

DC %>% group_by(BEDRM) %>% 
  summarize(`Average Price (millions of dollars)` = mean(PRICE)/1000000, .groups = 'drop') %>% 
  kable(digits = 3)
```

\vfill

\newpage

\vfill

### Regression Model

There are many factors in this dataset that can are useful to predict housing prices.

\vfill
\begin{equation}
y_i = \beta_0 + \beta_1 * x_{SQFT,i} + \beta_2 x_{BEDRM,i} + \epsilon_i,
\end{equation}
where $y_i$ is the sales price of the $i^{th}$ house, $x_{SQFT,i}$ is the living square footage of the $i^{th}$ house, and $x_{BEDRM,i}$ is the number of bedrooms for the $i^{th}$ house. Note this implies that we are treating bedrooms as continuous variables as opposed to categorical.

\vfill

we usually write $\epsilon_i \sim N(0,\sigma^2)$. More on that soon.

\vfill

In R we often write something like: `price ~  LANDUSE + BEDRM`.

\vfill

Now let's write this model in matrix notation:

\begin{equation}
\underline{y} = X \underline{\beta} + \underline{\epsilon},
\end{equation}

where $\underline{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$, $X = \begin{bmatrix} 1 & x_{SQFT,1} & x_{BEDRM,1}\\
1 & x_{SQFT,2} & x_{BEDRM,2} \\ 
\vdots & \vdots & \vdots \\
1 & x_{SQFT,n} & x_{BEDRM,n}
\end{bmatrix}$ ,$\underline{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}$, and $\underline{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}$

\vfill

Now what are the implications of:

$\epsilon_i \sim N(0,\sigma^2)$ or $\underline{\epsilon} \sim N(\underline{0}, \Sigma)$, where $$\Sigma = \sigma \times \begin{bmatrix} 1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots &\ddots  & \vdots\\
0 & 0 & \cdots & 1\\
\end{bmatrix}$$

\vfill

These are equivalent statements and both imply that $y_i$ and $y_j$ are conditionally independent given X. In other words, after controlling for predictors (ward, square footage), then the price of house $i$ gives us no additional information about price of house $j$.  



