---
title: "Linear Algebra Primer"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```

##### Matrices / Vectors

A matrix is an $n \times p$ object. Matrices are often denoted by a capital letter (or Greek symbol). A few common matrices will be

\vfill


\newpage

Vectors are essentially one-dimension vectors and will be denoted with an underline. We will assume vectors are $q \times 1$ dimension unless noted with a transpose.


\vfill

The transpose operator will be denoted by $\underline{y}^T = \begin{pmatrix} y_1 & y_2 & \cdots & y_n \end{pmatrix}$ or $\underline{y}^{'}$, both of which would result in a $1 \times n$ vector.

\vfill
\newpage

##### Matrix Multiplication

The most important component in matrix multiplication is tracking dimensions.

\vfill

Consider a simple case with 

$$\hat{\underline{y}} = X \times \hat{\underline{\beta}},$$

where $X$ is a $2 \times 2$ matrix, $\begin{pmatrix} 1 & 2 \\ 1 & -1   \end{pmatrix}$ and $\hat{\underline{\beta}} = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$.

\vfill

\vfil



In R, we use `%*%` for matrix multiplication.

```{r}
X <- matrix(c(1,2, 1 ,-1), nrow = 2, ncol = 2, byrow = T); X
#X <- matrix(c(1, 1, 2 ,-1), nrow = 2, ncol = 2); X
beta_hat <- matrix(c(3,2),nrow =2, ncol = 1); beta_hat
```

\vfill

```{r}
y_hat <- X %*% beta_hat; y_hat
```

\vfill

\newpage

#### Kronecker Product

Kronecker product $\otimes$, enables a different type of matrix multiplication.

If $$A = \begin{bmatrix} 1 & 2\end{bmatrix}$$ and $B = \begin{bmatrix} 3 \\ 4\\ 5 \end{bmatrix}$, then


\vfill

## More about matrices

#### Nonsingular matrix and Matrix Inverse

If X is a square matrix, then X is nonsingular if there exists another matrix s.t.

$X X^{-1} = I$ and $X^{-1} X = I$

\vfill

$X^{-1}$ is the inverse of a matrix. We can calculate the inverse of a matrix for a $1 \times 1$ matrix, perhaps as $2 \times 2$, matrix and maybe even a $3 \times 3$ matrix. However, beyond that it is quite challenging and time consuming. Furthermore, it is also (relatively) time intensive for your computer.

\vfill

#### Orthogonal matrices

If a matrix $X$ has an inverse that is also the transpose, $X X^T = I$, then $X$ is an orthogonal matrix.

\vfill


\newpage

##### Motivating Dataset: Washington (DC) housing dataset

Hopefully the connections to statistics are clear, using $X$ and $\beta$, but let's consider a motivating dataset.

\vfill
This dataset contains housing information from Washington, D.C. It was used for a STAT532 exam, so apologize in advance for any scar tissue.

```{r}
DC <- read_csv('https://math.montana.edu/ahoegh/teaching/stat532/data/DC.csv')
```


```{r}
DC %>% group_by(WARD) %>% 
  summarize(`Average Price (millions of dollars)` = mean(PRICE)/1000000, .groups = 'drop') %>% 
  kable(digits = 3)

DC %>% group_by(BEDRM) %>% 
  summarize(`Average Price (millions of dollars)` = mean(PRICE)/1000000, .groups = 'drop') %>% 
  kable(digits = 3)
```

\vfill

\newpage

\vfill

### Regression Model

There are many factors in this dataset that can are useful to predict housing prices.

\vfill
\begin{equation}
y_i = \beta_0 + \beta_1 * x_{SQFT,i} + \beta_2 x_{BEDRM,i} + \epsilon_i,
\end{equation}
where $y_i$ is the sales price of the $i^{th}$ house, $x_{SQFT,i}$ is the living square footage of the $i^{th}$ house, and $x_{BEDRM,i}$ is the number of bedrooms for the $i^{th}$ house. Note this implies that we are treating bedrooms as continuous variables as opposed to categorical.

\vfill

we usually write $\epsilon_i \sim N(0,\sigma^2)$. More on that soon.

\vfill

In R we often write something like: `price ~  LANDUSE + BEDRM`.

\vfill

Now let's write this model in matrix notation:

\begin{equation}
\underline{y} = X \underline{\beta} + \underline{\epsilon},
\end{equation}

where $\underline{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$, $X = \begin{bmatrix} 1 & x_{SQFT,1} & x_{BEDRM,1}\\
1 & x_{SQFT,2} & x_{BEDRM,2} \\ 
\vdots & \vdots & \vdots \\
1 & x_{SQFT,n} & x_{BEDRM,n}
\end{bmatrix}$ ,$\underline{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}$, and $\underline{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}$

\vfill

\vfill